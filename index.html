<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE">
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <style type="text/css">
    @import url(https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300);

    /* Color scheme stolen from Sergey Karayev */
    a {
      /*color: #b60a1c;*/
      color: #1772d0;
      /*color: #bd0a36;*/
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Roboto', sans-serif;
      font-size: 15px;
      font-weight: 300;
    }

    strong {
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      /*font-family: 'Avenir Next';*/
      font-size: 15px;
      font-weight: 400;
    }

    heading {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
      font-size: 24px;
      font-weight: 400;
    }

    papertitle {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
      font-size: 15px;
      font-weight: 500;
    }

    name {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      font-weight: 400;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 140px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="media/website_icon.png">
  <title>Arash Akbari - Homepage</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
    type='text/css'>
  <script src="script/functions.js"></script>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="80%" valign="middle">
              <p align="center">
                <name>Arash Akbari</name>
              <div id="profile-email" align="center">akbari dot ara at m dot northeastern dot edu</div>
              </p>
              <p>
                I'm a second-year Ph.D. student in Computer Engineering at Northeastern University, 
                where I conduct research on efficient deep learning and generative models. 
                My work focuses on developing novel techniques to make state-of-the-art models more practical 
                for real-world deployment, and I am proudly supervised by Prof. Yanzhi Wang. 
                Before beginning my doctoral studies, I earned my Bachelor's degree in Computer Science from the 
                University of Tehran. 
              </p>
              <p>
                I was a research intern at <a href="http://www.shlab.org.cn/">CNRS</a> with <a
                  href="https://scholar.google.com/citations?user=ahUibskAAAAJ">Dr. Xuelong Li</a> and at CISPA with <a>Dr. Xiao Zhang</a>. 
              </p>
              
              <p align=center>
                <a href="mailto:dlqu22@m.fudan.edu.cn">Email</a> &nbsp|&nbsp
                <!-- <a href="files/qudelin-en.pdf">CV</a> &nbsp|&nbsp -->
                <a href="https://github.com/DelinQu">GitHub</a> &nbsp|&nbsp
                <a href="https://scholar.google.com/citations?user=zgiFoOwAAAAJ">Google Scholar</a> &nbsp|&nbsp
                <a href="https://www.linkedin.com/in/%E5%BE%B7%E6%9E%97-%E5%B1%88-8b9975280"> LinkedIn</a> &nbsp|&nbsp
                <a href="https://x.com/delin_qu23107">Twitter</a> &nbsp|&nbsp
                <a href="https://www.xiaohongshu.com/user/profile/61f22e8f0000000010007002">Xhs</a>
              </p>
            </td>
            <td width="20%">
              <img src="media/prof.JPG" width="200" height="200" alt="headshot" style="border-radius:50%;object-fit:cover;aspect-ratio:1/1;transform:scale(1.3);">
            </td>
          </tr>
        </table>

        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="10%" valign="middle">
              <a href="https://ethz.ch/en.html"><img src="media/eth_logo.png" width="120"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://www.is.mpg.de/"><img src="media/mpi_logo.png" width="60"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://research.google/"><img src="media/google_logo.png" width="55"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://about.facebook.com/realitylabs/"><img src="media/frl_logo.png" width="60"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://www.inria.fr/en/"><img src="media/inria_logo.jpg" width="90"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://vision.in.tum.de"><img src="media/tum_logo.jpg" width="60"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://www.vibot.org"><img src="media/vibot_logo_transparent.png" width="35"></a>
            </td>
          </tr>
        </table> -->

        

        <!-- News  -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>News</heading>
              <ul>
                <li><strong>Sep 2025</strong>
                  We introduce <img src="media/eo_logo_bare.png" width="15"> <a
                  href="http://eo-robotics.ai"><strong>EO-1</strong></a>
                  , an open-source unified embodied foundation model trained on Interleaved Embodied Multimodal Data, integrates discrete auto-regressive decoding with continuous flow matching for multimodal embodied reasoning and robot control.
                  <strong>EO-Robotic Team: <a href="http://eo-robotics.ai">http://eo-robotics.ai</a></strong>.
                </li>

                <li><strong>Apr 2025</strong><img src="media/logo_oral.jpg" width="20"><a
                    href="https://spatialvla.github.io/">Our papers <strong>SpatialVLA</strong></a> are accepted to
                  <strong>Robotics: Science and Systems 2025</strong>. I’ll be presenting a short <strong><span style="color:#c20000;">Spotlight Talk</span></strong> in RSS, LA. Meanwhile, I’m actively exploring <strong>internships or visiting opportunities</strong>. Open to exciting collaborations—DM or email me!
                </li>
                <li><strong>Mar 2025</strong>: I am awarded the <a href="">Top Outstanding
                    Ph.D. Student Scholarship of Fudan University</a> (top 1%).</li>
                <li><strong>Feb 2025</strong> Our Lifelong Robot paper <a href="">Think Small, Act Big</a> is accepted at CVPR
                  2025. </li>
                <li><strong>Mar 2025</strong> I will also be a research interning at <a href="https://agibot.com/">AgiBot</a> this
                  summer on Foundation Vision-language-action models. </li>
                <li><strong>Nov 2024</strong> I have secured the <strong style='color:#c20000;'>National Natural Science
                    Foundation of
                    China (NSFC)</strong> grant to support my research. Deeply grateful to my <a
                    href="https://scholar.google.com/citations?user=ahUibskAAAAJ">supervisor</a> and <a
                    href="https://github.com/SHAILAB-IPEC">research team@IPEC</a> </li>
                <li><strong>Dec 2024</strong><img src="media/logo_oral.jpg" width="20"><a
                    href="https://goxq.github.io/mifag/">Our paper <strong>Learning 2D Invariant Affordance Knowledge for 3D Affordance Grounding</strong></a> is accepted to
                  <strong>AAAI 2025</strong> as <strong><span style="color:#c20000;">Oral Paper</span> (top 4.6%)</strong>.
                </li>
                <li><strong>Sep 2024</strong> Our paper <a href="https://livescenes.github.io/">LiveScene
                    Language Embedding Interactive Radiance Fields for Physical Scene Rendering and Control</a> is
                  accepted at Neurips 2024. </li>
                <li><strong>May 2024</strong>: We integrated KAN into NeRF and conducted a preliminary evaluation, <a
                    href="https://t.co/5cus4thPrT">integrated KAN into NeRF 😆</a>, <a
                    href="https://github.com/Tavish9/KANeRF">Hands-On NeRF with KAN</a>!</li>
                <li><strong>Feb 2024</strong><img src="media/logo_oral.jpg" width="20"><a
                    href="https://gs-slam.github.io/">Our papers <strong>GS-SLAM</strong></a> and <a
                    href="https://delinqu.github.io/EN-SLAM/"><strong>EN-SLAM</strong></a> are both accepted to
                  <strong>CVPR
                    2024</strong> as <strong><span style="color:#c20000;">Two Highlight Paper</span> (top 2.6% ×
                    2)</strong>.
                </li>
                <li><strong>Jun 2023</strong>: Our paper <a
                    href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qu_Towards_Nonlinear-Motion-Aware_and_Occlusion-Robust_Rolling_Shutter_Correction_ICCV_2023_paper.pdf">Towards
                    Nonlinear-Motion-Aware and Occlusion-Robust Rolling Shutter Correction</a> got accepted to ICCV
                  2023</li>
                <li><strong>Jun 2023</strong>:<img src="media/logo_oral.jpg" width="20">The extension of my
                  undergraduate thesis <a href="https://ieeexplore.ieee.org/document/10148802">Fast Rolling Shutter
                    Correction in the Wild</a>
                  got accepted to <span style="color:#c20000;"><strong>TPAMI</strong></span>!</li>
                <li><strong>Mar 2023</strong>: Our paper <a
                    href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_Revisiting_Rolling_Shutter_Bundle_Adjustment_Toward_Accurate_and_Fast_Solution_CVPR_2023_paper.pdf">Revisiting
                    Rolling Shutter Bundle Adjustment: Toward Accurate and Fast Solution</a> got accepted to CVPR
                  2023.</li>
                <li><strong>Sep 2022</strong>: Start my PhD journey at <a href="https://www.fudan.edu.cn/en">Fudan
                    University</a>!</li>

                <!-- 🔥 NOTE: below is old news  -->
                <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>

                <div id="old_news" style="display: none;">
                  <li><strong>Aug 2022</strong>: I start my research internship at <a
                      href="http://www.shlab.org.cn/">Shanghai AI Laboratory @ EPIC</a> with <a
                      href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Prof. Bin Zhao</a>!
                  </li>
                  <li><strong>Jul 2022</strong>: Graduated from <a href="https://www-en.hnu.edu.cn">Hunan University</a>
                    in computer science and technology, Bachelor, Ranking 1/208, with National Scholarship and
                    Outstanding Graduate.
                  </li>
                </div>
              </ul>
            </td>
          </tr>
        </table>
        <!-- Research  -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <heading>Research</heading>
          <tr onmouseout="eo1_stop()" onmouseover="eo1_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='eo1_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202509-eo1/video.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202509-eo1/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function eo1_start() {
                  document.getElementById('eo1_shape').style.opacity = "1";
                }
                function eo1_stop() {
                  document.getElementById('eo1_shape').style.opacity = "0";
                }
                eo1_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="http://eo-robotics.ai/eo-1">
                <papertitle>EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control,</papertitle>
              </a>
              <strong>Delin Qu*</strong>,
              <a href="https://github.com/HaomingSong">Haoming Song*</a>,
              <a href="https://github.com/Tavish9">Qizhi Chen*</a>,
              <a href="">Zhaoqin Chen*</a>,
              <a href="https://scholar.google.com/citations?user=oZSREOkAAAAJ">Xianqiang Gao*</a>,
              <a href="https://scholar.google.com/citations?user=oqN1dA8AAAAJ">Guanghui Ren</a>,
              <a href="https://scholar.google.com/citations?user=N36nqu4AAAAJ">Maoqing Yao</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>,
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              et al. <strong><a href="http://eo-robotics.ai">EO-Robotic Team</a></strong>
              <br>
              <a href="http://eo-robotics.ai">
                <img
                  src="https://img.shields.io/badge/EO--Robotics-Website-5865F2?logo=googleplay&logoColor=white"
                  alt="EO-Robotics Website"
                />
              </a>
              <a href="https://arxiv.org/abs/2508.21112">
                <img
                  src="https://img.shields.io/badge/EO--1-Paper-red?logo=arxiv&logoColor=red"
                  alt="EO-Robotics Paper on arXiv"
                />
              </a>
              <a href="https://huggingface.co/IPEC-COMMUNITY/EO-1-3B">
                <img 
                    src="https://img.shields.io/badge/EO--1--3B-Model-FFCC11?logo=huggingface&logoColor=brightyellow" 
                    alt="EO-1 Model"
                />
              </a>
              <a href="https://huggingface.co/datasets/IPEC-COMMUNITY/EO-Data1.5M">
                <img
                  src="https://img.shields.io/badge/Dataset-EO--Data1.5M-brightgreen?logo=huggingface&logoColor=brightyellow"
                  alt="EO-1.5M"
                />
              </a>
              <br>
              An open-source unified embodied foundation model integrates discrete auto-regressive decoding with continuous flow matching for embodied reasoning and robot control.
            </td>
          </tr>



        </table>

        <!-- Mentored Students -->
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <heading>Mentored Students</heading>
          <tr>
            <td>
              I am fortunate to (co-)mentor some talented and highly motivated
              students. I
              have learnt from and gotten inspired by them:
              <ul>
                <li> <a href="https://janackermann.info/"><strong>Jan
                      Ackermann</strong></a> (Ongoing): MSc student at ETH
                  Zurich<br>
                  <ul>
                    <li> Semester thesis: Continual Learning of Gaussian Splatting
                      with
                      Local Optimization (CVPR'25 submission)
                    <li> &#8594; Master thesis at Stanford University, advised by <a
                        href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>
                  </ul>
                  <br>
                <li> <a href="https://www.linkedin.com/in/goncayilmaz/"><strong>Gonca
                      Yilmaz</strong></a> (2024): MSc student at University of
                  Zurich<br>
                  <ul>
                    <li> Semester thesis: Open Vocabulary Segmentation from
                      Multi-Modal
                      Inputs (ICCVW'23)
                    <li> Master thesis: <a href="https://open-das.github.io/">OpenDAS:
                        Open-Vocabulary Domain Adaption for Segmentation</a>
                      (ICLR'25
                      submission)
                    <li> &#8594; Software engineer at <a href="https://about.google/">Google</a>
                  </ul>
                  <br>
            </td>
          </tr>
        </table> -->

        <!-- Invited Talks -->
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:collapse;margin-right:auto;margin-left:auto;"> -->
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <heading>Invited Talks</heading>
          <br><br><br>
          <tr onmouseout="teleai_stop()" onmouseover="teleai_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='talk_teleai'>
                  <img src='talk/202503_teleai/teaser2.png' width="180" height="100">
                </div>
                <img src='talk/202503_teleai/teaser1.png' width="180" height="100">
              </div>
              </div>
              <script type="text/javascript">
                function teleai_start() {
                  document.getElementById('talk_teleai').style.opacity = "1";
                }
                function teleai_stop() {
                  document.getElementById('talk_teleai').style.opacity = "0";
                }
                teleai_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="talk/202503_teleai/spatialvla_slides.pdf">
                <papertitle>Exploring Spatial Representations for Visual-Language-Action Model</papertitle>
              </a>
              <br>
              <em><strong>Institute of Artificial Intelligence (TeleAI), China Telecom</strong></em>, hosted by <a
                href="https://baichenjia.github.io/">Chenjia Bai</a>, Mar 2025<br>
              <br>
              A spatial-enhanced vision-language-action model that is trained on 1.1 Million real robot episodes,
              toward the More Generalist Agents System.
              <a href="talk/202503_teleai/spatialvla_slides.pdf">slides</a>
            </td>
          </tr>
        </table> -->


        

  

  <!-- Academic Services -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <heading>Academic Services</heading>
    <tr>
      <td>
        <ul>
          <li><strong>Conference Reviewer</strong>: TPAMI, CVPR, ICCV, ECCV, ICLR, ICML, and NeurIPS.</li>
          <li><strong>2023 Spring</strong>: COMP130135.04 Object Oriented Programming, Teaching Assistant.</li>
          </li>
        </ul>
      </td>
    </tr>
  </table>
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td>
        <br>
        <p align="right">
          <font size="2">
            template adapted from <a href="https://jonbarron.info/">
              <font size="2">this awesome website</font>
            </a>
          </font>
        </p>
      </td>
    </tr>
  </table>
  <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
  </script>
  <script type="text/javascript">
    try {
      var pageTracker = _gat._getTracker("UA-116734954-1");
      pageTracker._trackPageview();
    } catch (err) { }
  </script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-116734954-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-116734954-1');
  </script>

  <!-- Visitor Map -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <heading>Visitor Map</heading>
    <tr>
      <td align="center">
        <div style="width: 100%; max-width: 600px; margin: 0 auto;">
          <!-- RevolverMaps Globe -->
          <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5x4x3x2x1x0x0x0x0x0&amp;m=0&amp;c=ff0000&amp;cr1=50000&amp;f=arial&amp;l=33" async="async"></script>
        </div>
        <p style="text-align: center; font-size: 12px; color: #666; margin-top: 10px;">
          <em>Real-time visitor locations from around the world</em>
        </p>
      </td>
    </tr>
  </table>

  </td>
  </tr>
  </table>
</body>

</html>