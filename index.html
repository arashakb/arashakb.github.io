<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE">
  <meta name=viewport content=â€œwidth=800â€>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <style type="text/css">
    @import url(https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300);

    /* Color scheme stolen from Sergey Karayev */
    a {
      /*color: #b60a1c;*/
      color: #1772d0;
      /*color: #bd0a36;*/
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Roboto', sans-serif;
      font-size: 15px;
      font-weight: 300;
    }

    strong {
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      /*font-family: 'Avenir Next';*/
      font-size: 15px;
      font-weight: 400;
    }

    heading {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
      font-size: 24px;
      font-weight: 400;
    }

    papertitle {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
      font-size: 15px;
      font-weight: 500;
    }

    name {
      /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
      font-family: 'Roboto', sans-serif;
      /*font-family: 'Avenir Next';*/
      font-weight: 400;
      font-size: 32px;
    }

    .one {
      width: 160px;
      height: 140px;
      position: relative;
    }

    .two {
      width: 160px;
      height: 160px;
      position: absolute;
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="media/website_icon.png">
  <title>Arash Akbari - Homepage</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
    type='text/css'>
  <script src="script/functions.js"></script>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="80%" valign="middle">
              <p align="center">
                <name>Arash Akbari</name>
              <div id="profile-email" align="center">akbari dot ara at m dot northeastern dot edu</div>
              </p>
              <p>
                I'm a second-year Ph.D. student in Computer Engineering at Northeastern University, 
                where I conduct research on efficient deep learning and generative models. 
                My work focuses on developing novel techniques to make state-of-the-art models more practical 
                for real-world deployment, and I am proudly supervised by Prof. Yanzhi Wang. 
                Before beginning my doctoral studies, I earned my Bachelor's degree in Computer Science from the 
                University of Tehran. 
              </p>
              <p>
                I was a research intern at <a href="http://www.shlab.org.cn/">CNRS</a> with <a
                  href="https://scholar.google.com/citations?user=ahUibskAAAAJ">Dr. Xuelong Li</a> and at CISPA with <a>Dr. Xiao Zhang</a>. 
              </p>
              
              <p align=center>
                <a href="mailto:dlqu22@m.fudan.edu.cn">Email</a> &nbsp|&nbsp
                <!-- <a href="files/qudelin-en.pdf">CV</a> &nbsp|&nbsp -->
                <a href="https://github.com/DelinQu">GitHub</a> &nbsp|&nbsp
                <a href="https://scholar.google.com/citations?user=zgiFoOwAAAAJ">Google Scholar</a> &nbsp|&nbsp
                <a href="https://www.linkedin.com/in/%E5%BE%B7%E6%9E%97-%E5%B1%88-8b9975280"> LinkedIn</a> &nbsp|&nbsp
                <a href="https://x.com/delin_qu23107">Twitter</a> &nbsp|&nbsp
                <a href="https://www.xiaohongshu.com/user/profile/61f22e8f0000000010007002">Xhs</a>
              </p>
            </td>
            <td width="20%">
              <img src="media/Profile.heic" width="200" height="200" alt="headshot" style="border-radius:50%;object-fit:cover;aspect-ratio:1/1;">
            </td>
          </tr>
        </table>

        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="10%" valign="middle">
              <a href="https://ethz.ch/en.html"><img src="media/eth_logo.png" width="120"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://www.is.mpg.de/"><img src="media/mpi_logo.png" width="60"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://research.google/"><img src="media/google_logo.png" width="55"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://about.facebook.com/realitylabs/"><img src="media/frl_logo.png" width="60"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://www.inria.fr/en/"><img src="media/inria_logo.jpg" width="90"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://vision.in.tum.de"><img src="media/tum_logo.jpg" width="60"></a>
            </td>
            <td width="10%" valign="middle">
              <a href="https://www.vibot.org"><img src="media/vibot_logo_transparent.png" width="35"></a>
            </td>
          </tr>
        </table> -->


        <!-- News  -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <heading>News</heading>
              <ul>
                <li><strong>Sep 2025</strong>
                  We introduce <img src="media/eo_logo_bare.png" width="15"> <a
                  href="http://eo-robotics.ai"><strong>EO-1</strong></a>
                  , an open-source unified embodied foundation model trained on Interleaved Embodied Multimodal Data, integrates discrete auto-regressive decoding with continuous flow matching for multimodal embodied reasoning and robot control.
                  <strong>EO-Robotic Team: <a href="http://eo-robotics.ai">http://eo-robotics.ai</a></strong>.
                </li>

                <li><strong>Apr 2025</strong><img src="media/logo_oral.jpg" width="20"><a
                    href="https://spatialvla.github.io/">Our papers <strong>SpatialVLA</strong></a> are accepted to
                  <strong>Robotics: Science and Systems 2025</strong>. Iâ€™ll be presenting a short <strong><span style="color:#c20000;">Spotlight Talk</span></strong> in RSS, LA. Meanwhile, Iâ€™m actively exploring <strong>internships or visiting opportunities</strong>. Open to exciting collaborationsâ€”DM or email me!
                </li>
                <li><strong>Mar 2025</strong>: I am awarded the <a href="">Top Outstanding
                    Ph.D. Student Scholarship of Fudan University</a> (top 1%).</li>
                <li><strong>Feb 2025</strong> Our Lifelong Robot paper <a href="">Think Small, Act Big</a> is accepted at CVPR
                  2025. </li>
                <li><strong>Mar 2025</strong> I will also be a research interning at <a href="https://agibot.com/">AgiBot</a> this
                  summer on Foundation Vision-language-action models. </li>
                <li><strong>Nov 2024</strong> I have secured the <strong style='color:#c20000;'>National Natural Science
                    Foundation of
                    China (NSFC)</strong> grant to support my research. Deeply grateful to my <a
                    href="https://scholar.google.com/citations?user=ahUibskAAAAJ">supervisor</a> and <a
                    href="https://github.com/SHAILAB-IPEC">research team@IPEC</a> </li>
                <li><strong>Dec 2024</strong><img src="media/logo_oral.jpg" width="20"><a
                    href="https://goxq.github.io/mifag/">Our paper <strong>Learning 2D Invariant Affordance Knowledge for 3D Affordance Grounding</strong></a> is accepted to
                  <strong>AAAI 2025</strong> as <strong><span style="color:#c20000;">Oral Paper</span> (top 4.6%)</strong>.
                </li>
                <li><strong>Sep 2024</strong> Our paper <a href="https://livescenes.github.io/">LiveScene
                    Language Embedding Interactive Radiance Fields for Physical Scene Rendering and Control</a> is
                  accepted at Neurips 2024. </li>
                <li><strong>May 2024</strong>: We integrated KAN into NeRF and conducted a preliminary evaluation, <a
                    href="https://t.co/5cus4thPrT">integrated KAN into NeRF ðŸ˜†</a>, <a
                    href="https://github.com/Tavish9/KANeRF">Hands-On NeRF with KAN</a>!</li>
                <li><strong>Feb 2024</strong><img src="media/logo_oral.jpg" width="20"><a
                    href="https://gs-slam.github.io/">Our papers <strong>GS-SLAM</strong></a> and <a
                    href="https://delinqu.github.io/EN-SLAM/"><strong>EN-SLAM</strong></a> are both accepted to
                  <strong>CVPR
                    2024</strong> as <strong><span style="color:#c20000;">Two Highlight Paper</span> (top 2.6% Ã—
                    2)</strong>.
                </li>
                <li><strong>Oct 2023</strong>: I am awarded the <a href="https://www.tencent.com/en-us/">Tencent
                    Scholarship</a> (top 0.1%).</li>
                <li><strong>Jun 2023</strong>: Our paper <a
                    href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qu_Towards_Nonlinear-Motion-Aware_and_Occlusion-Robust_Rolling_Shutter_Correction_ICCV_2023_paper.pdf">Towards
                    Nonlinear-Motion-Aware and Occlusion-Robust Rolling Shutter Correction</a> got accepted to ICCV
                  2023</li>
                <li><strong>Jun 2023</strong>:<img src="media/logo_oral.jpg" width="20">The extension of my
                  undergraduate thesis <a href="https://ieeexplore.ieee.org/document/10148802">Fast Rolling Shutter
                    Correction in the Wild</a>
                  got accepted to <span style="color:#c20000;"><strong>TPAMI</strong></span>!</li>
                <li><strong>Mar 2023</strong>: Our paper <a
                    href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_Revisiting_Rolling_Shutter_Bundle_Adjustment_Toward_Accurate_and_Fast_Solution_CVPR_2023_paper.pdf">Revisiting
                    Rolling Shutter Bundle Adjustment: Toward Accurate and Fast Solution</a> got accepted to CVPR
                  2023.</li>
                <li><strong>Sep 2022</strong>: Start my PhD journey at <a href="https://www.fudan.edu.cn/en">Fudan
                    University</a>!</li>

                <!-- ðŸ”¥ NOTE: below is old news  -->
                <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>

                <div id="old_news" style="display: none;">
                  <li><strong>Aug 2022</strong>: I start my research internship at <a
                      href="http://www.shlab.org.cn/">Shanghai AI Laboratory @ EPIC</a> with <a
                      href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Prof. Bin Zhao</a>!
                  </li>
                  <li><strong>Jul 2022</strong>: Graduated from <a href="https://www-en.hnu.edu.cn">Hunan University</a>
                    in computer science and technology, Bachelor, Ranking 1/208, with National Scholarship and
                    Outstanding Graduate.
                  </li>
                </div>
              </ul>
            </td>
          </tr>
        </table>
        <!-- Research  -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <heading>Research</heading>
          <tr onmouseout="eo1_stop()" onmouseover="eo1_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='eo1_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202509-eo1/video.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202509-eo1/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function eo1_start() {
                  document.getElementById('eo1_shape').style.opacity = "1";
                }
                function eo1_stop() {
                  document.getElementById('eo1_shape').style.opacity = "0";
                }
                eo1_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="http://eo-robotics.ai/eo-1">
                <papertitle>EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control,</papertitle>
              </a>
              <strong>Delin Qu*</strong>,
              <a href="https://github.com/HaomingSong">Haoming Song*</a>,
              <a href="https://github.com/Tavish9">Qizhi Chen*</a>,
              <a href="">Zhaoqin Chen*</a>,
              <a href="https://scholar.google.com/citations?user=oZSREOkAAAAJ">Xianqiang Gao*</a>,
              <a href="https://scholar.google.com/citations?user=oqN1dA8AAAAJ">Guanghui Ren</a>,
              <a href="https://scholar.google.com/citations?user=N36nqu4AAAAJ">Maoqing Yao</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>,
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              et al. <strong><a href="http://eo-robotics.ai">EO-Robotic Team</a></strong>
              <br>
              <a href="http://eo-robotics.ai">
                <img
                  src="https://img.shields.io/badge/EO--Robotics-Website-5865F2?logo=googleplay&logoColor=white"
                  alt="EO-Robotics Website"
                />
              </a>
              <a href="https://arxiv.org/abs/2508.21112">
                <img
                  src="https://img.shields.io/badge/EO--1-Paper-red?logo=arxiv&logoColor=red"
                  alt="EO-Robotics Paper on arXiv"
                />
              </a>
              <a href="https://huggingface.co/IPEC-COMMUNITY/EO-1-3B">
                <img 
                    src="https://img.shields.io/badge/EO--1--3B-Model-FFCC11?logo=huggingface&logoColor=brightyellow" 
                    alt="EO-1 Model"
                />
              </a>
              <a href="https://huggingface.co/datasets/IPEC-COMMUNITY/EO-Data1.5M">
                <img
                  src="https://img.shields.io/badge/Dataset-EO--Data1.5M-brightgreen?logo=huggingface&logoColor=brightyellow"
                  alt="EO-1.5M"
                />
              </a>
              <br>
              An open-source unified embodied foundation model integrates discrete auto-regressive decoding with continuous flow matching for embodied reasoning and robot control.
            </td>
          </tr>


         

          

          <tr onmouseout="livescene_stop()" onmouseover="livescene_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='livescene_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202409-livescene/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202409-livescene/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function livescene_start() {
                  document.getElementById('livescene_shape').style.opacity = "1";
                }
                function livescene_stop() {
                  document.getElementById('livescene_shape').style.opacity = "0";
                }
                livescene_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://livescenes.github.io/">
                <papertitle>
                  LiveScene: Language Embedding Interactive Radiance Fields for Physical Scene Rendering and Control
                </papertitle>
              </a>
              <br>
              <strong>Delin Qu*</strong>,
              <a href="https://github.com/Tavish9">Qizhi Chen*</a>,
              <a href="https://github.com/zhangpingrui">Pingrui Zhang</a>,
              <a href="https://scholar.google.com/citations?user=oZSREOkAAAAJ">Xianqiang Gao</a>,
              <!-- <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>, -->
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              <!-- <a href="https://scholar.google.com/citations?user=cw3EaAYAAAAJ">Zhigang Wang</a>, -->
              <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en">Xuelong Li</a>,
              <br>
              <em>Conference on Neural Information Processing Systems (<strong>Neurips</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2406.16038">paper</a> |
              <a href="https://livescenes.github.io">project page</a> |
              <a href="https://livescenes.github.io">video</a> |
              <a href="https://github.com/Tavish9/livescene">code</a> |
              <a href="https://huggingface.co/datasets/IPEC-COMMUNITY/LiveScene">dataset <img alt="Static Badge"
                  src="https://img.shields.io/badge/ðŸ¤— Huggingface-livescene-green"></a>
              <br>
              Embedding language feature to interactive scenes, grounding and manipulating interactable objects with
              language instructions.
            </td>
          </tr>

          <tr onmouseout="qrsc_stop()" onmouseover="qrsc_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='qrsc_shape'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="publication/202406-qrst/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src='publication/202406-qrst/teaser.png' width="160" height="120">
              </div>
              <script type="text/javascript">
                function qrsc_start() {
                  document.getElementById('qrsc_shape').style.opacity = "1";
                }
                function qrsc_stop() {
                  document.getElementById('qrsc_shape').style.opacity = "0";
                }
                qrsc_stop()
              </script>
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="https://delinqu.github.io/QRSC">
                <papertitle>
                  Towards Nonlinear-Motion-Aware and Occlusion-Robust Rolling Shutter Correction
                </papertitle>
              </a>
              <br>
              <strong>Delin Qu*</strong>,
              <a href="https://yizhenlao.github.io">Yizhen Lao</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=DQB0hqwAAAAJ">Bin Zhao</a>,
              <a href="https://scholar.google.com/citations?user=cw3EaAYAAAAJ">Zhigang Wang</a>,
              <a href="https://scholar.google.es/citations?user=dasL9V4AAAAJ">Dong Wang</a>,
              <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=en">Xuelong Li</a>,
              <br>
              <em>Proceedings of the IEEE/CVF International Conference on Computer Vision(<strong>ICCV</strong>)</em>,
              2023
              <br>
              <a
                href="https://openaccess.thecvf.com/content/ICCV2023/papers/Qu_Towards_Nonlinear-Motion-Aware_and_Occlusion-Robust_Rolling_Shutter_Correction_ICCV_2023_paper.pdf">paper</a>
              |
              <a href="https://delinqu.github.io/QRSC/">project page</a> |
              <a href="https://www.youtube.com/watch?v=Or-yvKHUrZ0">video</a> |
              <a href="https://github.com/DelinQu/qrsc?tab=readme-ov-file">code</a>
              <br>
              A geometry-based Quadratic Rolling Shutter (QRS) motion solver, which precisely estimates the high-order
              correction field of individual pixels.
            </td>
          </tr>

          
            <td valign="top" width="75%">
              <a href="https://delinqu.github.io/NW-RSBA">
                <papertitle>
                  Revisiting Rolling Shutter Bundle Adjustment: Toward Accurate and Fast Solution
                </papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=0z2qluIAAAAJ">Bangyan Liao*</a>,
              <strong>Delin Qu*</strong>,
              <a href="https://dblp.org/pid/58/6739.html">Yifei Xue</a>,
              <a href="https://github.com/Kikihqq">Huiqing Zhang</a>,
              <a href="https://yizhenlao.github.io">Yizhen Lao</a>.
              <br>
              <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a
                href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liao_Revisiting_Rolling_Shutter_Bundle_Adjustment_Toward_Accurate_and_Fast_Solution_CVPR_2023_paper.pdf">paper</a>
              |
              <a href="https://delinqu.github.io/NW-RSBA">project page</a> |
              <a href="https://www.youtube.com/watch?v=aCo60XUatss">video</a> |
              <a href="https://github.com/DelinQu/NW-RSBA">code</a>
              <br>
              An accurate and fast bundle adjustment solution that estimates the 6-DoF pose with an independent RS model
              of the camera and the geometry of the environment based on measurements from a rolling shutter camera.
            </td>
          </tr>
        </table>

        <!-- Mentored Students -->
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <heading>Mentored Students</heading>
          <tr>
            <td>
              I am fortunate to (co-)mentor some talented and highly motivated
              students. I
              have learnt from and gotten inspired by them:
              <ul>
                <li> <a href="https://janackermann.info/"><strong>Jan
                      Ackermann</strong></a> (Ongoing): MSc student at ETH
                  Zurich<br>
                  <ul>
                    <li> Semester thesis: Continual Learning of Gaussian Splatting
                      with
                      Local Optimization (CVPR'25 submission)
                    <li> &#8594; Master thesis at Stanford University, advised by <a
                        href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>
                  </ul>
                  <br>
                <li> <a href="https://www.linkedin.com/in/goncayilmaz/"><strong>Gonca
                      Yilmaz</strong></a> (2024): MSc student at University of
                  Zurich<br>
                  <ul>
                    <li> Semester thesis: Open Vocabulary Segmentation from
                      Multi-Modal
                      Inputs (ICCVW'23)
                    <li> Master thesis: <a href="https://open-das.github.io/">OpenDAS:
                        Open-Vocabulary Domain Adaption for Segmentation</a>
                      (ICLR'25
                      submission)
                    <li> &#8594; Software engineer at <a href="https://about.google/">Google</a>
                  </ul>
                  <br>
            </td>
          </tr>
        </table> -->

        <!-- Invited Talks -->
        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:collapse;margin-right:auto;margin-left:auto;"> -->
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <heading>Invited Talks</heading>
          <br><br><br>
          <tr onmouseout="teleai_stop()" onmouseover="teleai_start()">
            <td width="25%">
              <div class="one">
                <div class="two" id='talk_teleai'>
                  <img src='talk/202503_teleai/teaser2.png' width="180" height="100">
                </div>
                <img src='talk/202503_teleai/teaser1.png' width="180" height="100">
              </div>
              </div>
              <script type="text/javascript">
                function teleai_start() {
                  document.getElementById('talk_teleai').style.opacity = "1";
                }
                function teleai_stop() {
                  document.getElementById('talk_teleai').style.opacity = "0";
                }
                teleai_stop()
              </script>
            </td>
            <td valign="top" width="75%">
              <a href="talk/202503_teleai/spatialvla_slides.pdf">
                <papertitle>Exploring Spatial Representations for Visual-Language-Action Model</papertitle>
              </a>
              <br>
              <em><strong>Institute of Artificial Intelligence (TeleAI), China Telecom</strong></em>, hosted by <a
                href="https://baichenjia.github.io/">Chenjia Bai</a>, Mar 2025<br>
              <br>
              A spatial-enhanced vision-language-action model that is trained on 1.1 Million real robot episodes,
              toward the More Generalist Agents System.
              <a href="talk/202503_teleai/spatialvla_slides.pdf">slides</a>
            </td>
          </tr>
        </table> -->


        <!-- Selected Projects -->
        <!-- <table width="100%" align="center" border="0" cellpadding="20" cellspacing="0">
          <heading>Selected Projects</heading>
          <br><br><br>
          <tr>
            <td width="25%">
              <div class="one">
                <div class="two" id='fastumi'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="project/fast-umi/demo.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://fastumi.com">
                <papertitle>
                  <img src="media/logo_oral.jpg" width="15"> FastUMI: A Scalable and Hardware-Independent Universal
                  Manipulation Interface with Dataset
                </papertitle>
              </a>
              <br>
              Zhaxizhuoma, Kehui Liu, et.al, <strong>Delin Qu</strong>, Dong Wang, Yan Ding, Bin Zhao, Xuelong Li
              <!-- <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2025 <strong>(<spanstyle="color:#c20000;">Oral</span>, top 1.8%)</strong>
              <br> -->
              <a href="https://arxiv.org/abs/2409.19499">paper</a> |
              <a href="https://fastumi.com/">project page</a> |
              <a href="https://fastumi.com/">video</a> |
              <a href="https://github.com/RealRobotSquad/FastUMI_Data">code</a>
              <p></p>
              A substantial redesign of the Universal Manipulation Interface system enabling rapid deployment and
              delivering robust performance in real-world data acquisition.
              <p></p>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two" id='embllm'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="project/ood/ood.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="http://scis.scichina.com/en/2024/124201.pdf">
                <papertitle>
                  Optics-driven drone
                </papertitle>
              </a>
              <br>
              Xuelong Li, Guan Huang, Zhigang Wang, <strong>Delin Qu</strong>, Bin Zhao
              <br>
              <em>Science China. Information Sciences, 67(2), 124201</em>, 2024
              <br>
              <a href="http://scis.scichina.com/en/2024/124201.pdf">paper</a> |
              <a href="http://scis.scichina.com/en/2024/124201.pdf">project page</a> |
              <a href="http://scis.scichina.com/en/2024/124201.pdf">video</a> |
              <a href="http://scis.scichina.com/en/2024/124201.pdf">code</a>
              <p></p>
              A remote charging technology for drones to enhance their autonomy and intelligence during mission
              execution
              <p></p>
            </td>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two" id='embllm'>
                  <video width="160" height="120" muted autoplay loop>
                    <source src="project/embllm/embllm.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="http://scis.scichina.com/en/2024/124201.pdf">
                <papertitle>
                  Large Model Heterogeneous Intelligent Agent Systems
                </papertitle>
              </a>
              <br>
              Kehui Liu, Zixin Tang, et.al, <strong>Delin Qu</strong>, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li
              <br>
              <em>International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2409.15146v2">paper</a> |
              <a href="https://github.com/mrkeee/coherent">project page</a> |
              <a href="https://www.youtube.com/watch?v=dV1J-VXdEJA">video</a> |
              <a href="https://github.com/mrkeee/coherent">code</a>
              <p></p>
              A novel LLM-based task planning framework for collaboration of heterogeneous multi-robot systems including
              quadrotors, robotic dogs, and robotic arms.
              <p></p>
            </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                <div class="two" id='any4lerobot'>
                  <!-- <video width="160" height="120" muted autoplay loop>
                    <source src="project/any4lerobot/video.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video> -->
                  <img src="project/any4lerobot/teaser.png" width="160" height="120">
                </div>
              </div>
            </td>
            <td valign="top" width="75%">
              <a href="https://github.com/Tavish9/any4lerobot">
                <papertitle>
                  Any4LeRobot: A tool collection for LeRobot
                </papertitle>
              </a>
              <br>
              <a href="https://github.com/Tavish9/any4lerobot">paper</a> |
              <a href="https://github.com/Tavish9/any4lerobot">project page</a> |
              <a href="https://github.com/Tavish9/any4lerobot">video</a> |
              <a href="https://github.com/Tavish9/any4lerobot">code</a>
              <p></p>
              A curated collection of utilities for LeRobot Projects, including data conversion scripts, preprocessing tools, training workflow helpers and etc.
              <p></p>
            </td>
          </tr>
    </tr>

  </table> -->

  <!-- Honors & Awards -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <heading>Honors & Awards</heading>
    <tr>
      <td>
        <ul>
          <li><strong>Sep 2022 - Now</strong>: Top Outstanding PhD Student Scholarship of Fudan University in
            2025, Tencent Scholarship in 2023, Fudan University Master's Excellence Scholarship in 2022,
            Outstanding Student Award in 2023, Fudan University's Outstanding Youth League Member in 2024.</li>
          <li><strong>Sep 2018 - Jun 2022</strong>: National Scholarship in 2021, National Scholarship in 2020,
            National Inspirational Scholarship in 2019, Finalist Prize of Mathematical Contest in Modeling, Second
            Prize of Asia-Pacific Mathematical Contest in Modeling, Second Prize in National Internet of Things
            Design Contest, Second Prize in Internet Competition of Hunan Province, Excellence Award in the Huawei
            AI Cloud Cup, Huawei College Scholarship, Huawei Smart Base Future Star, Excellent Graduation Thesis.
          </li>
        </ul>
      </td>
    </tr>
  </table>

  <!-- Academic Services -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <heading>Academic Services</heading>
    <tr>
      <td>
        <ul>
          <li><strong>Conference Reviewer</strong>: TPAMI, CVPR, ICCV, ECCV, ICLR, ICML, and NeurIPS.</li>
          <li><strong>2023 Spring</strong>: COMP130135.04 Object Oriented Programming, Teaching Assistant.</li>
          </li>
        </ul>
      </td>
    </tr>
  </table>
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr>
      <td>
        <br>
        <p align="right">
          <font size="2">
            template adapted from <a href="https://jonbarron.info/">
              <font size="2">this awesome website</font>
            </a>
          </font>
        </p>
      </td>
    </tr>
  </table>
  <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
  </script>
  <script type="text/javascript">
    try {
      var pageTracker = _gat._getTracker("UA-116734954-1");
      pageTracker._trackPageview();
    } catch (err) { }
  </script>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-116734954-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-116734954-1');
  </script>
  </td>
  </tr>
  </table>
</body>

</html>